{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Youtube_v1_data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJENNPTCZMpI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "b7457044-194a-4939-bd01-eb69749b802a"
      },
      "source": [
        "!rm -rf wiki_libs\n",
        "!git clone https://github.com/tsonic/wiki_libs.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'wiki_libs'...\n",
            "remote: Enumerating objects: 84, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/84)\u001b[K\rremote: Counting objects:   2% (2/84)\u001b[K\rremote: Counting objects:   3% (3/84)\u001b[K\rremote: Counting objects:   4% (4/84)\u001b[K\rremote: Counting objects:   5% (5/84)\u001b[K\rremote: Counting objects:   7% (6/84)\u001b[K\rremote: Counting objects:   8% (7/84)\u001b[K\rremote: Counting objects:   9% (8/84)\u001b[K\rremote: Counting objects:  10% (9/84)\u001b[K\rremote: Counting objects:  11% (10/84)\u001b[K\rremote: Counting objects:  13% (11/84)\u001b[K\rremote: Counting objects:  14% (12/84)\u001b[K\rremote: Counting objects:  15% (13/84)\u001b[K\rremote: Counting objects:  16% (14/84)\u001b[K\rremote: Counting objects:  17% (15/84)\u001b[K\rremote: Counting objects:  19% (16/84)\u001b[K\rremote: Counting objects:  20% (17/84)\u001b[K\rremote: Counting objects:  21% (18/84)\u001b[K\rremote: Counting objects:  22% (19/84)\u001b[K\rremote: Counting objects:  23% (20/84)\u001b[K\rremote: Counting objects:  25% (21/84)\u001b[K\rremote: Counting objects:  26% (22/84)\u001b[K\rremote: Counting objects:  27% (23/84)\u001b[K\rremote: Counting objects:  28% (24/84)\u001b[K\rremote: Counting objects:  29% (25/84)\u001b[K\rremote: Counting objects:  30% (26/84)\u001b[K\rremote: Counting objects:  32% (27/84)\u001b[K\rremote: Counting objects:  33% (28/84)\u001b[K\rremote: Counting objects:  34% (29/84)\u001b[K\rremote: Counting objects:  35% (30/84)\u001b[K\rremote: Counting objects:  36% (31/84)\u001b[K\rremote: Counting objects:  38% (32/84)\u001b[K\rremote: Counting objects:  39% (33/84)\u001b[K\rremote: Counting objects:  40% (34/84)\u001b[K\rremote: Counting objects:  41% (35/84)\u001b[K\rremote: Counting objects:  42% (36/84)\u001b[K\rremote: Counting objects:  44% (37/84)\u001b[K\rremote: Counting objects:  45% (38/84)\u001b[K\rremote: Counting objects:  46% (39/84)\u001b[K\rremote: Counting objects:  47% (40/84)\u001b[K\rremote: Counting objects:  48% (41/84)\u001b[K\rremote: Counting objects:  50% (42/84)\u001b[K\rremote: Counting objects:  51% (43/84)\u001b[K\rremote: Counting objects:  52% (44/84)\u001b[K\rremote: Counting objects:  53% (45/84)\u001b[K\rremote: Counting objects:  54% (46/84)\u001b[K\rremote: Counting objects:  55% (47/84)\u001b[K\rremote: Counting objects:  57% (48/84)\u001b[K\rremote: Counting objects:  58% (49/84)\u001b[K\rremote: Counting objects:  59% (50/84)\u001b[K\rremote: Counting objects:  60% (51/84)\u001b[K\rremote: Counting objects:  61% (52/84)\u001b[K\rremote: Counting objects:  63% (53/84)\u001b[K\rremote: Counting objects:  64% (54/84)\u001b[K\rremote: Counting objects:  65% (55/84)\u001b[K\rremote: Counting objects:  66% (56/84)\u001b[K\rremote: Counting objects:  67% (57/84)\u001b[K\rremote: Counting objects:  69% (58/84)\u001b[K\rremote: Counting objects:  70% (59/84)\u001b[K\rremote: Counting objects:  71% (60/84)\u001b[K\rremote: Counting objects:  72% (61/84)\u001b[K\rremote: Counting objects:  73% (62/84)\u001b[K\rremote: Counting objects:  75% (63/84)\u001b[K\rremote: Counting objects:  76% (64/84)\u001b[K\rremote: Counting objects:  77% (65/84)\u001b[K\rremote: Counting objects:  78% (66/84)\u001b[K\rremote: Counting objects:  79% (67/84)\u001b[K\rremote: Counting objects:  80% (68/84)\u001b[K\rremote: Counting objects:  82% (69/84)\u001b[K\rremote: Counting objects:  83% (70/84)\u001b[K\rremote: Counting objects:  84% (71/84)\u001b[K\rremote: Counting objects:  85% (72/84)\u001b[K\rremote: Counting objects:  86% (73/84)\u001b[K\rremote: Counting objects:  88% (74/84)\u001b[K\rremote: Counting objects:  89% (75/84)\u001b[K\rremote: Counting objects:  90% (76/84)\u001b[K\rremote: Counting objects:  91% (77/84)\u001b[K\rremote: Counting objects:  92% (78/84)\u001b[K\rremote: Counting objects:  94% (79/84)\u001b[K\rremote: Counting objects:  95% (80/84)\u001b[K\rremote: Counting objects:  96% (81/84)\u001b[K\rremote: Counting objects:  97% (82/84)\u001b[K\rremote: Counting objects:  98% (83/84)\u001b[K\rremote: Counting objects: 100% (84/84)\u001b[K\rremote: Counting objects: 100% (84/84), done.\u001b[K\n",
            "remote: Compressing objects:   1% (1/57)\u001b[K\rremote: Compressing objects:   3% (2/57)\u001b[K\rremote: Compressing objects:   5% (3/57)\u001b[K\rremote: Compressing objects:   7% (4/57)\u001b[K\rremote: Compressing objects:   8% (5/57)\u001b[K\rremote: Compressing objects:  10% (6/57)\u001b[K\rremote: Compressing objects:  12% (7/57)\u001b[K\rremote: Compressing objects:  14% (8/57)\u001b[K\rremote: Compressing objects:  15% (9/57)\u001b[K\rremote: Compressing objects:  17% (10/57)\u001b[K\rremote: Compressing objects:  19% (11/57)\u001b[K\rremote: Compressing objects:  21% (12/57)\u001b[K\rremote: Compressing objects:  22% (13/57)\u001b[K\rremote: Compressing objects:  24% (14/57)\u001b[K\rremote: Compressing objects:  26% (15/57)\u001b[K\rremote: Compressing objects:  28% (16/57)\u001b[K\rremote: Compressing objects:  29% (17/57)\u001b[K\rremote: Compressing objects:  31% (18/57)\u001b[K\rremote: Compressing objects:  33% (19/57)\u001b[K\rremote: Compressing objects:  35% (20/57)\u001b[K\rremote: Compressing objects:  36% (21/57)\u001b[K\rremote: Compressing objects:  38% (22/57)\u001b[K\rremote: Compressing objects:  40% (23/57)\u001b[K\rremote: Compressing objects:  42% (24/57)\u001b[K\rremote: Compressing objects:  43% (25/57)\u001b[K\rremote: Compressing objects:  45% (26/57)\u001b[K\rremote: Compressing objects:  47% (27/57)\u001b[K\rremote: Compressing objects:  49% (28/57)\u001b[K\rremote: Compressing objects:  50% (29/57)\u001b[K\rremote: Compressing objects:  52% (30/57)\u001b[K\rremote: Compressing objects:  54% (31/57)\u001b[K\rremote: Compressing objects:  56% (32/57)\u001b[K\rremote: Compressing objects:  57% (33/57)\u001b[K\rremote: Compressing objects:  59% (34/57)\u001b[K\rremote: Compressing objects:  61% (35/57)\u001b[K\rremote: Compressing objects:  63% (36/57)\u001b[K\rremote: Compressing objects:  64% (37/57)\u001b[K\rremote: Compressing objects:  66% (38/57)\u001b[K\rremote: Compressing objects:  68% (39/57)\u001b[K\rremote: Compressing objects:  70% (40/57)\u001b[K\rremote: Compressing objects:  71% (41/57)\u001b[K\rremote: Compressing objects:  73% (42/57)\u001b[K\rremote: Compressing objects:  75% (43/57)\u001b[K\rremote: Compressing objects:  77% (44/57)\u001b[K\rremote: Compressing objects:  78% (45/57)\u001b[K\rremote: Compressing objects:  80% (46/57)\u001b[K\rremote: Compressing objects:  82% (47/57)\u001b[K\rremote: Compressing objects:  84% (48/57)\u001b[K\rremote: Compressing objects:  85% (49/57)\u001b[K\rremote: Compressing objects:  87% (50/57)\u001b[K\rremote: Compressing objects:  89% (51/57)\u001b[K\rremote: Compressing objects:  91% (52/57)\u001b[K\rremote: Compressing objects:  92% (53/57)\u001b[K\rremote: Compressing objects:  94% (54/57)\u001b[K\rremote: Compressing objects:  96% (55/57)\u001b[K\rremote: Compressing objects:  98% (56/57)\u001b[K\rremote: Compressing objects: 100% (57/57)\u001b[K\rremote: Compressing objects: 100% (57/57), done.\u001b[K\n",
            "remote: Total 84 (delta 43), reused 60 (delta 25), pack-reused 0\u001b[K\n",
            "Unpacking objects:   1% (1/84)   \rUnpacking objects:   2% (2/84)   \rUnpacking objects:   3% (3/84)   \rUnpacking objects:   4% (4/84)   \rUnpacking objects:   5% (5/84)   \rUnpacking objects:   7% (6/84)   \rUnpacking objects:   8% (7/84)   \rUnpacking objects:   9% (8/84)   \rUnpacking objects:  10% (9/84)   \rUnpacking objects:  11% (10/84)   \rUnpacking objects:  13% (11/84)   \rUnpacking objects:  14% (12/84)   \rUnpacking objects:  15% (13/84)   \rUnpacking objects:  16% (14/84)   \rUnpacking objects:  17% (15/84)   \rUnpacking objects:  19% (16/84)   \rUnpacking objects:  20% (17/84)   \rUnpacking objects:  21% (18/84)   \rUnpacking objects:  22% (19/84)   \rUnpacking objects:  23% (20/84)   \rUnpacking objects:  25% (21/84)   \rUnpacking objects:  26% (22/84)   \rUnpacking objects:  27% (23/84)   \rUnpacking objects:  28% (24/84)   \rUnpacking objects:  29% (25/84)   \rUnpacking objects:  30% (26/84)   \rUnpacking objects:  32% (27/84)   \rUnpacking objects:  33% (28/84)   \rUnpacking objects:  34% (29/84)   \rUnpacking objects:  35% (30/84)   \rUnpacking objects:  36% (31/84)   \rUnpacking objects:  38% (32/84)   \rUnpacking objects:  39% (33/84)   \rUnpacking objects:  40% (34/84)   \rUnpacking objects:  41% (35/84)   \rUnpacking objects:  42% (36/84)   \rUnpacking objects:  44% (37/84)   \rUnpacking objects:  45% (38/84)   \rUnpacking objects:  46% (39/84)   \rUnpacking objects:  47% (40/84)   \rUnpacking objects:  48% (41/84)   \rUnpacking objects:  50% (42/84)   \rUnpacking objects:  51% (43/84)   \rUnpacking objects:  52% (44/84)   \rUnpacking objects:  53% (45/84)   \rUnpacking objects:  54% (46/84)   \rUnpacking objects:  55% (47/84)   \rUnpacking objects:  57% (48/84)   \rUnpacking objects:  58% (49/84)   \rUnpacking objects:  59% (50/84)   \rUnpacking objects:  60% (51/84)   \rUnpacking objects:  61% (52/84)   \rUnpacking objects:  63% (53/84)   \rUnpacking objects:  64% (54/84)   \rUnpacking objects:  65% (55/84)   \rUnpacking objects:  66% (56/84)   \rUnpacking objects:  67% (57/84)   \rUnpacking objects:  69% (58/84)   \rUnpacking objects:  70% (59/84)   \rUnpacking objects:  71% (60/84)   \rUnpacking objects:  72% (61/84)   \rUnpacking objects:  73% (62/84)   \rUnpacking objects:  75% (63/84)   \rUnpacking objects:  76% (64/84)   \rUnpacking objects:  77% (65/84)   \rUnpacking objects:  78% (66/84)   \rUnpacking objects:  79% (67/84)   \rUnpacking objects:  80% (68/84)   \rUnpacking objects:  82% (69/84)   \rUnpacking objects:  83% (70/84)   \rUnpacking objects:  84% (71/84)   \rUnpacking objects:  85% (72/84)   \rUnpacking objects:  86% (73/84)   \rUnpacking objects:  88% (74/84)   \rUnpacking objects:  89% (75/84)   \rUnpacking objects:  90% (76/84)   \rUnpacking objects:  91% (77/84)   \rUnpacking objects:  92% (78/84)   \rUnpacking objects:  94% (79/84)   \rUnpacking objects:  95% (80/84)   \rUnpacking objects:  96% (81/84)   \rUnpacking objects:  97% (82/84)   \rUnpacking objects:  98% (83/84)   \rUnpacking objects: 100% (84/84)   \rUnpacking objects: 100% (84/84), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFhOD5agRhg9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import json\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "from zipfile import ZipFile\n",
        "import pickle\n",
        "import gc\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "import itertools\n",
        "import importlib\n",
        "import wiki_libs\n",
        "from wiki_libs.preprocessing import *\n",
        "from functools import partial\n",
        "import torch.optim as optim\n",
        "import traceback\n",
        "importlib.reload(wiki_libs.preprocessing)\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "# cudnn.benchmark = True # CUDA for PyTorch. Faster runtime if input size constant in iterations.  \n",
        "!pip install ipdb > /dev/null\n",
        "!pip install line_profiler > /dev/null\n",
        "%load_ext line_profiler\n",
        "import ipdb\n",
        "import pdb"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qtGOBgkoLR4k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e236f0b3-9d3a-400e-954d-c8d6a4bdee92"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.flush_and_unmount()\n",
        "drive.mount('/content/gdrive',force_remount=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClPh1xtBXlwh",
        "colab_type": "text"
      },
      "source": [
        "Configuration Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRen2hlaY5RJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "df_cp = read_category_links()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwy0G71ahWi2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "#load ngram model\n",
        "\n",
        "ngram_model = load_ngram_model(NGRAM_MODEL_PATH_PREFIX + \"title_category_ngram_model.pickle\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciMMs_Ry1Y-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(df_cp.columns)\n",
        "print(df_cp.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g34lta9TiMGT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_cp['page_title'].isnull().mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShwZU4q3f1KN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# tranform title, return list of list of ngrams\n",
        "title_transformed, category_transformed = get_transformed_title_category(ngram_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpN1c9G53lvT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_title = generate_vocab(title_transformed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAMczYgU4Cmp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_category = generate_vocab(category_transformed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ROzLwcq7Rfh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_title_category = generate_vocab(title_transformed + category_transformed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQh_ZrV59Auz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# with open('gdrive/My Drive/Projects with Wei/wiki_data/ngram_model/vocab.json','w') as f:\n",
        "#   json.dump({\n",
        "#       'vocab_title':vocab_title,\n",
        "#       'vocab_category':vocab_category,\n",
        "#       'vocab_title_category':vocab_title_category,\n",
        "#       }), f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVWyb8bYZ0T7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('gdrive/My Drive/Projects with Wei/wiki_data/ngram_model/vocab.json','r') as f:\n",
        "    vocab_json = json.load(f)\n",
        "    vocab_title = vocab_json['vocab_title']\n",
        "    vocab_category = vocab_json['vocab_category']\n",
        "    vocab_title_category = 'vocab_title_category'\n",
        "    word2ind_title_category = defaultdict(lambda:-1, {w:i for i, w in enumerate(vocab_title_category)})\n",
        "    word2ind_title = defaultdict(lambda:-1, {w:i for i, w in enumerate(vocab_title)})\n",
        "    word2ind_category = defaultdict(lambda:-1, {w:i for i, w in enumerate(vocab_category)})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjJvyCTB62qD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(vocab_title)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQKzz9_gghw7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ngram_to_idx(l, word2ind):\n",
        "  return np.array([word2ind[ng] for ng in l])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln-c2CjWbjva",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "df_page_2_bow = (\n",
        "    df_cp[['page_id','page_title']]\n",
        "    .drop_duplicates()\n",
        "    .fillna({'page_title':''})\n",
        "    .assign(processed_title = lambda df: df['page_title'].apply(process_title))\n",
        "    .assign(processed_title_ngram = lambda df: \n",
        "            transform_ngram(df['processed_title'].tolist(), ngram_model))\n",
        "    .assign(processed_title_ngram_idx = lambda df: \n",
        "            df['processed_title_ngram'].apply(ngram_to_idx, word2ind=word2ind_title))\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44iHEGxkiLMs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "df_category_2_bow = (\n",
        "    df_cp[['page_category']]\n",
        "    .drop_duplicates()\n",
        "    .fillna({'page_category':''})\n",
        "    .assign(processed_category = lambda df: df['page_category']\n",
        "                                      .apply(process_title))\n",
        "    .assign(processed_category_ngram = lambda df: transform_ngram(df['processed_category'].tolist(), ngram_model))\n",
        "    .assign(processed_category_ngram_idx = \n",
        "                                     lambda df: df['processed_category_ngram'].apply(ngram_to_idx, word2ind=word2ind_category))\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5foEZOu7aEtE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "df_lp = next(read_link_pairs_chunks(n_chunk=10))\n",
        "df_lp.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrvrVwqyVPvl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Config():\n",
        "    #TODO: fill in the dirs\n",
        "    training_dir = \\\n",
        "    testing_dir = \\\n",
        "\n",
        "    #TODO: complete hyper-par defs\n",
        "    EMD_DIM1      = 2 ** 3 #left tower \n",
        "    EMD_DIM2      = 2 ** 3 #right tower (= left for L2 cals)\n",
        "    MAX_EPOCHS    = 100\n",
        "    LEARNING_RATE = 1 ** (-5) # grid \\in [-6:-3]\n",
        "    NUM_EPOCHS     = 10\n",
        "\n",
        "    # data parameters. to be used in the DataLoader caclass\n",
        "    data_params = {'BATCH_SIZE': 2 ** 6, # grid \\in [3:8]\n",
        "                   'shuffle': True,\n",
        "                   'num_workers': 6}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPbFLyVXL5yw",
        "colab_type": "text"
      },
      "source": [
        "Helper_functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-a6DdTHtL7Dr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_plot(iteration, loss):\n",
        "    plt.plot(iteration, loss)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4wpwC9oS6Ua",
        "colab_type": "text"
      },
      "source": [
        "**Import** data\n",
        "\n",
        "Reference: https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7KSzeuRIyxG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PageWordStats(object):\n",
        "    def __init__(self, read_path,\n",
        "                 ngram_model_file = \"title_category_ngram_model.pickle\",\n",
        "                 output_path = \"gdrive/My Drive/Projects with Wei/wiki_data/page_word_stats.json\",\n",
        "                 n_chunk = 10,\n",
        "                 ):\n",
        "        if read_path is not None:\n",
        "            config = json.load(open(read_path, 'r'))\n",
        "            self.word_frequency = config['word_frequency']\n",
        "            self.word2id = config['word2id']\n",
        "            self.id2word = config['id2word']\n",
        "            self.page_frequency = {int(k):v for k, v in config['page_frequency'].items()}\n",
        "\n",
        "            self.page2id = {int(k):v for k, v in config['page2id'].items()}\n",
        "            self.id2page = config['id2page']\n",
        "        else:\n",
        "            # recomptue the target page stats\n",
        "\n",
        "            gen = read_link_pairs_chunks(n_chunk = n_chunk)\n",
        "            print('generating page id stats...')\n",
        "            s_page = []\n",
        "            for df_chunk in gen:\n",
        "                val_counts = df_chunk['page_id_target'].append(df_chunk['page_id_source']).value_counts()\n",
        "                s_page.append(val_counts)\n",
        "            df_stats = (\n",
        "                pd.concat(s_page)\n",
        "                .rename_axis(\"page_id\")  #rename index\n",
        "                .to_frame('count')\n",
        "                .groupby(\"page_id\")\n",
        "                    .sum()\n",
        "                .reset_index()\n",
        "            )\n",
        "            self.page_frequency = {row.page_id: row.count \n",
        "                                   for row in df_stats.itertuples(index = False)}\n",
        "            self.page2id = {p:i for i, p in enumerate(df_stats['page_id'])}\n",
        "            self.id2page = df_stats['page_id'].tolist()\n",
        "\n",
        "            # recompute the word stats\n",
        "            print('generating word/ngram stats from title and categories...')\n",
        "\n",
        "            ngram_model = load_ngram_model(NGRAM_MODEL_PATH_PREFIX + ngram_model_file)\n",
        "            title_transformed, category_transformed = get_transformed_title_category(ngram_model)\n",
        "            s_words = (\n",
        "                pd.Series(itertools.chain(*(title_transformed + category_transformed)))\n",
        "                .value_counts()\n",
        "            )\n",
        "            # word_frequency is a list, where ith element is the word frequency of word with id i.\n",
        "            self.word_frequency = s_words.tolist()\n",
        "            self.word2id = {w:i for i, w in enumerate(s_words.index)}\n",
        "            self.id2word = s_words.index.tolist()\n",
        "            json.dump({\n",
        "                'word_frequency': self.word_frequency,\n",
        "                'word2id': self.word2id,\n",
        "                'id2word':self.id2word,\n",
        "                'page_frequency': self.page_frequency,\n",
        "                'page2id': self.page2id,\n",
        "                'id2page': self.id2page,\n",
        "                }, \n",
        "                open(output_path, 'w'))\n",
        "        print('There are %d unique words/ngrams' % len(self.word2id))\n",
        "        print('There are %d unique pages' % len(self.page2id))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dP-jKZ8qaNJL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %%time\n",
        "# page_word_stats = PageWordStats(read_path = None,n_chunk=10)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bp80ZKWKbXJu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "19ac8163-9af4-4657-ddda-41c35416d615"
      },
      "source": [
        "page_word_stats = PageWordStats(read_path = \"gdrive/My Drive/Projects with Wei/wiki_data/page_word_stats.json\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1748542 unique words/ngrams\n",
            "There are 5330812 unique pages\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFSjeu9OSpGp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NEGATIVE_TABLE_SIZE = 1e8\n",
        "class WikiDataset(torch.utils.data.IterableDataset):\n",
        "    'Characterizes a dataset for PyTorch'\n",
        "    def __init__(self, file_list, compression, n_chunk, num_negs, page_word_stats, ns_exponent):\n",
        "        'Initialization'\n",
        "        #self.labels = labels\n",
        "        # self.list_IDs = list_IDs\n",
        "        # if isinstance(file_list, str):\n",
        "        #     file_list = [file_list]\n",
        "        self.file_list = file_list\n",
        "        self.compression = compression\n",
        "        self.n_chunk = n_chunk\n",
        "        self.pos = 0\n",
        "\n",
        "        self.chunk_iterator = None\n",
        "        self.instance_dict = None\n",
        "\n",
        "\n",
        "        self.negatives = []\n",
        "        self.negpos = 0\n",
        "        self.num_negs = num_negs\n",
        "        self.page_word_stats = page_word_stats\n",
        "\n",
        "        self.word_frequency = page_word_stats.word_frequency\n",
        "        self.word2id = page_word_stats.word2id\n",
        "        self.id2word = page_word_stats.id2word\n",
        "        self.page2id = page_word_stats.page2id\n",
        "        p,i = zip(*self.page2id.items())\n",
        "        self.page2id_series_map = pd.Series(i, index = p, dtype = np.int64)\n",
        "        self.id2page = page_word_stats.id2page\n",
        "        self.page_frequency = page_word_stats.page_frequency\n",
        "        self.initTableNegatives(ns_exponent=ns_exponent)\n",
        "\n",
        "    # Iterable may not know the length of the stream before hand\n",
        "    # def __len__(self):\n",
        "    #     'Denotes the total number of samples'\n",
        "    #     return len(self.list_IDs)\n",
        "\n",
        "    # def __getitem__(self, index):\n",
        "    #     'Generates one sample of data'\n",
        "    #     # Select sample\n",
        "    #     ID = self.list_IDs[index]\n",
        "\n",
        "    #     # Load data and get label\n",
        "    #     X = torch.load('data/' + ID + '.pt')\n",
        "    #     y = self.labels[ID]\n",
        "\n",
        "    #     return X, y\n",
        "    \n",
        "    def __iter__(self):\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self.chunk_iterator is None:\n",
        "            self.chunk_iterator = read_files_in_chunks(self.file_list, compression=self.compression, \n",
        "                                                       n_chunk = self.n_chunk, progress_bar = True)\n",
        "            \n",
        "        if self.instance_dict is None or self.pos >= len(self.instance_dict):\n",
        "            #print('read_new_chunk', flush = True)\n",
        "            df = next(self.chunk_iterator)\n",
        "            # have to reset self.pos after pull next in above iterator, otherwise \n",
        "            # it does not invalidate current instance_dict after iterator is exhausted.\n",
        "            self.pos = 0\n",
        "\n",
        "            df = df.assign(\n",
        "                    page_id_source = lambda df: df['page_id_source'].map(self.page2id_series_map),\n",
        "                    page_id_target = lambda df: df['page_id_target'].map(self.page2id_series_map),\n",
        "                )\n",
        "\n",
        "            self.instance_dict = list(df.itertuples(index = False, name = None))\n",
        "        ret = self.instance_dict[self.pos]\n",
        "        self.pos += 1\n",
        "        return ret\n",
        "\n",
        "    def getNegatives(self, target, size):  # TODO check equality with target\n",
        "        response = self.negatives[self.negpos:self.negpos + size]\n",
        "        # reshuffle negative table if negpos > total neg table size.\n",
        "        if (self.negpos + size) // NEGATIVE_TABLE_SIZE >= 1:\n",
        "            print('reshuffle negative table...')\n",
        "            np.random.shuffle(self.negatives)\n",
        "        self.negpos = (self.negpos + size) % len(self.negatives)\n",
        "        if len(response) != size:\n",
        "            return np.concatenate((response, self.negatives[0:self.negpos]))\n",
        "        return response\n",
        "\n",
        "    def initTableNegatives(self, ns_exponent):\n",
        "        print('Initializing negative samples', flush=True)\n",
        "        page_id, page_counts = zip(*self.page_frequency.items())\n",
        "        ratio = np.array(page_counts).astype(np.float64) ** ns_exponent / sum(page_counts)\n",
        "        sampled_count = np.round(ratio * NEGATIVE_TABLE_SIZE)\n",
        "\n",
        "        df = pd.DataFrame.from_records(enumerate(sampled_count))\n",
        "        # the column 0 is the page id, column 1 is the count of the page\n",
        "        self.negatives = np.repeat(df[0].astype(np.int64).values, df[1].astype(np.int64).values)\n",
        "        np.random.shuffle(self.negatives)\n",
        "\n",
        "    def collate(self,batches):\n",
        "        negs = self.getNegatives(None, self.num_negs * len(batches)).reshape((len(batches), self.num_negs))\n",
        "        id_list, positive_list = zip(*batches)\n",
        "\n",
        "        return torch.LongTensor(id_list), torch.LongTensor(positive_list), torch.from_numpy(negs)\n",
        "\n",
        "    @staticmethod\n",
        "    def worker_init_fn(worker_id, file_handle_lists):\n",
        "        worker_info = torch.utils.data.get_worker_info()\n",
        "        dataset = worker_info.dataset  # the dataset copy in this worker process\n",
        "        worker_id = worker_info.id\n",
        "        dataset.file_list = file_handle_lists[worker_id]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrhHiSSMHyev",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%pdb"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMsdT7kmCXDB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test():\n",
        "    tt = get_file_handles_in_zip(LINK_PAIRS_LOCATION)\n",
        "    for i, d in enumerate(WikiDataset(tt, compression='zip', n_chunk=100, num_negs=5, page_word_stats = page_word_stats, ns_exponent = 0.5)):\n",
        "        print(d)\n",
        "        if i > 100:\n",
        "            break\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFuCRj7X4jyw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%lprun -f WikiDataset.__next__ test()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeFXyLw2S8Mk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Datasets\n",
        "# partition = # IDs\n",
        "# labels = # Labels\n",
        "\n",
        "# #dataloader -> iterator\n",
        "# training_set = Dataset(partition['train'], labels)\n",
        "# validation_set = Dataset(partition['validation'], labels)\n",
        "\n",
        "# #TODO: load trainset and testset\n",
        "# training_generator  = DataLoader(trainset, **data_params)\n",
        "# validation_generator  = DataLoader(testset, **data_params)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8F1ILBCXSvz",
        "colab_type": "text"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htHjJZE7cgib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class two_tower(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size, hidden_dim1, out_dim):\n",
        "        super(nn.Module, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(context_size * embedding_dim, hidden_dim1)\n",
        "        self.linear2 = nn.Linear(hidden_dim1, out_dim)\n",
        "\n",
        "    def forward_left(self, inputs):\n",
        "        embeds = self.embeddings(inputs).view((1, -1))\n",
        "        h = F.relu(self.linear1(embeds))\n",
        "        out = F.relu(self.linear2(h))\n",
        "        # log_probs = F.log_softmax(out, dim=1)\n",
        "        return out\n",
        "\n",
        "    def forward_right(self, inputs):\n",
        "        embeds = self.embeddings(inputs).view((1, -1))\n",
        "        h = F.relu(self.linear1(embeds))\n",
        "        out = F.relu(self.linear2(h))\n",
        "        # log_probs = F.log_softmax(out, dim=1)\n",
        "        return out\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        inputs_left, inputs_right = inputs\n",
        "        return self.forward_left(inputs_left), self.forward_right(inputs_right)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lskV0o6b5yOm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class OneTower(nn.Module):\n",
        "    def __init__(self, corpus_size, embedding_dim, hidden_dim1, out_dim, sparse, single_layer = False):\n",
        "        super(OneTower, self).__init__()\n",
        "        self.embeddings = nn.Embedding(corpus_size, embedding_dim, sparse=sparse)\n",
        "        self.single_layer = single_layer\n",
        "\n",
        "        # if single_layer is True, it essentially become w2v model with single hidden layer\n",
        "        if not self.single_layer:\n",
        "            self.linear1 = nn.Linear(embedding_dim, hidden_dim1)\n",
        "            self.linear2 = nn.Linear(hidden_dim1, out_dim)\n",
        "        self.out_embeddings = nn.Embedding(corpus_size, embedding_dim, sparse=sparse)\n",
        "        \n",
        "    def forward(self, pos_u, pos_v, neg_v):\n",
        "        emb_u = self.embeddings(pos_u)\n",
        "        emb_v = self.out_embeddings(pos_v)\n",
        "        emb_neg_v = self.out_embeddings(neg_v)\n",
        "\n",
        "        if not self.single_layer:\n",
        "            h1 = F.relu(self.linear1(emb_u))\n",
        "            h2 = F.relu(self.linear2(h1))\n",
        "        else:\n",
        "            h2 = emb_v\n",
        "\n",
        "        score = torch.sum(torch.mul(h2, emb_v), dim=1)\n",
        "        score = torch.clamp(score, max=10, min=-10)\n",
        "        score = -F.logsigmoid(score)\n",
        "\n",
        "\n",
        "        neg_score = torch.bmm(emb_neg_v, h2.unsqueeze(2)).squeeze()\n",
        "        neg_score = torch.clamp(neg_score, max=10, min=-10)\n",
        "        neg_score = -torch.sum(F.logsigmoid(-neg_score), dim=1)\n",
        "\n",
        "        return torch.mean(score + neg_score)    "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns7DMTP32Cl-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fe0d6a8e-8a35-43f9-c92c-8825372acd8a"
      },
      "source": [
        "%pdb"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Automatic pdb calling has been turned ON\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XL7lfyCsDkIv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultipleOptimizer:\n",
        "    def __init__(self, *op):\n",
        "        self.optimizers = op\n",
        "\n",
        "    def zero_grad(self):\n",
        "        for op in self.optimizers:\n",
        "            op.zero_grad()\n",
        "\n",
        "    def step(self):\n",
        "        for op in self.optimizers:\n",
        "            op.step()\n",
        "\n",
        "class WikiTrainer:\n",
        "\n",
        "    def __init__(self, page_word_stats, hidden_dim1, out_dim, embedding_dim=100, batch_size=32, window_size=5, iterations=3,\n",
        "                 initial_lr=0.001, min_count=12, num_workers=0, collate_fn='custom', iprint=500, t=1e-3, ns_exponent=0.75, \n",
        "                 optimizer='adam', optimizer_kwargs = None, warm_start_model = None, lr_schedule = False, timeout = 60, n_chunk = 20,\n",
        "                 sparse=False, single_layer=False, test = False, save_embedding = True):\n",
        "\n",
        "        file_handle_lists = get_files_in_dir(LINK_PAIRS_LOCATION)\n",
        "        if test:\n",
        "            file_handle_lists = file_handle_lists[:2]\n",
        "            num_workers = 0\n",
        "            n_chunk = 1\n",
        "            \n",
        "        dataset = WikiDataset(file_list = None, compression = 'zip', n_chunk = n_chunk, \n",
        "                              page_word_stats = page_word_stats, num_negs=5, \n",
        "                              ns_exponent=ns_exponent)\n",
        "        if collate_fn == 'custom':\n",
        "            collate_fn = dataset.collate\n",
        "        else:\n",
        "            collate_fn = None\n",
        "        \n",
        "        if num_workers > 0:\n",
        "            file_handle_lists = np.array_split(file_handle_lists, num_workers)\n",
        "        else:\n",
        "            timeout = 0\n",
        "            dataset.file_list = file_handle_lists\n",
        "\n",
        "        self.dataloader = DataLoader(dataset, batch_size=batch_size,\n",
        "                                     shuffle=False, num_workers=num_workers, \n",
        "                                     collate_fn=collate_fn, \n",
        "                                     worker_init_fn=partial(dataset.worker_init_fn, file_handle_lists=file_handle_lists),\n",
        "                                     timeout = timeout,\n",
        "                                    )\n",
        "\n",
        "        # self.output_file_name = output_file\n",
        "        self.corpus_size = len(dataset.page_frequency)\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.save_embedding = save_embedding\n",
        "        self.iprint = iprint\n",
        "        self.batch_size = batch_size\n",
        "        self.iterations = iterations\n",
        "        self.initial_lr = initial_lr\n",
        "        self.model = OneTower(self.corpus_size, self.embedding_dim, \n",
        " #                             context_size = dataset.num_neg, \n",
        "                              hidden_dim1 = hidden_dim1, \n",
        "                              out_dim = out_dim, sparse=sparse,\n",
        "                              single_layer = single_layer,\n",
        "                              )\n",
        "\n",
        "        if warm_start_model is not None:\n",
        "            self.model.load_state_dict(torch.load(warm_start_model), strict=False)\n",
        "        self.optimizer = optimizer\n",
        "        if optimizer_kwargs is None:\n",
        "            optimizer_kwargs = {}\n",
        "        self.optimizer_kwargs = optimizer_kwargs\n",
        "        self.lr_schedule = lr_schedule\n",
        "        self.use_cuda = torch.cuda.is_available()\n",
        "        self.device = torch.device(\"cuda\" if self.use_cuda else \"cpu\")\n",
        "        if self.use_cuda:\n",
        "            self.model.cuda()\n",
        "\n",
        "    def train(self):\n",
        "        # clearn GPU memory cache\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        if self.optimizer == 'adam':\n",
        "            optimizer = optim.Adam(self.model.parameters(), lr=self.initial_lr, **self.optimizer_kwargs)\n",
        "        elif self.optimizer == 'sparse_adam':\n",
        "            optimizer = optim.SparseAdam(self.model.parameters(), lr=self.initial_lr, **self.optimizer_kwargs)\n",
        "        elif self.optimizer == 'sparse_dense_adam':\n",
        "            opti_sparse = optim.SparseAdam([self.model.embeddings.weight, self.model.out_embeddings.weight], lr=self.initial_lr, **self.optimizer_kwargs)\n",
        "            opti_dense = optim.Adam([self.model.linear1.weight, self.model.linear2.weight], lr=self.initial_lr, **self.optimizer_kwargs)\n",
        "            optimizer = MultipleOptimizer(opti_sparse, opti_dense)\n",
        "        elif self.optimizer == 'sgd':\n",
        "            optimizer = optim.SGD(self.model.parameters(), lr=self.initial_lr, **self.optimizer_kwargs)\n",
        "        elif self.optimizer == 'asgd':\n",
        "            optimizer = optim.ASGD(self.model.parameters(), lr=self.initial_lr, **self.optimizer_kwargs)\n",
        "        elif self.optimizer == 'adagrad':\n",
        "            optimizer = optim.Adagrad(self.model.parameters(), lr=self.initial_lr, **self.optimizer_kwargs)\n",
        "        else:\n",
        "            raise Exception('Unknown optimizer!')\n",
        "\n",
        "        for iteration in range(self.iterations):\n",
        "\n",
        "            print(\"\\n\\n\\nIteration: \" + str(iteration + 1))\n",
        "\n",
        "            if self.lr_schedule:\n",
        "                scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, len(self.dataloader))\n",
        "            running_loss = 0.0\n",
        "            iprint = self.iprint #len(self.dataloader) // 20\n",
        "            for i, sample_batched in enumerate(self.dataloader):\n",
        "                # ipdb.set_trace()\n",
        "                if len(sample_batched[0]) > 1:\n",
        "                    pos_u = sample_batched[0].to(self.device)\n",
        "                    pos_v = sample_batched[1].to(self.device)\n",
        "                    neg_v = sample_batched[2].to(self.device)\n",
        "                    \n",
        "                    optimizer.zero_grad()\n",
        "                    loss = self.model.forward(pos_u, pos_v, neg_v)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    if self.lr_schedule:\n",
        "                        scheduler.step()\n",
        "\n",
        "                    running_loss = running_loss * (1 - 5/iprint) + loss.item() * (5/iprint)\n",
        "                    if i > 0 and i % iprint == 0:\n",
        "                        # print(\" Loss: \" + str(running_loss) + ' lr: ' \n",
        "                        #     + str([param_group['lr'] for param_group in optimizer.param_groups]))\n",
        "                        print(\" Loss: \" + str(running_loss))\n",
        "            print(\" Loss: \" + str(running_loss))\n",
        "\n",
        "        #self.skip_gram_model.save_embedding(self.data.id2word, self.output_file_name)\n",
        "        if self.save_embedding:\n",
        "            print('Saving embeddings...', flush=True)\n",
        "            embeddings = self.model.embeddings.weight.cpu().data.numpy()\n",
        "            np.savez_compressed('gdrive/My Drive/Projects with Wei/wiki_data/wiki_embedding/embedding.npz', embeddings)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-fs7JuvM8HP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(epochs = 1, collate_fn='custom',num_workers=1, test = False, n_chunk = 20, save_embedding=True):\n",
        "  #ipdb.set_trace()\n",
        "  wt = WikiTrainer(\n",
        "                        page_word_stats = page_word_stats, \n",
        "                        hidden_dim1 = 100, \n",
        "                        out_dim = 100, \n",
        "                        # output_file=\"gdrive/My Drive/Projects with Wei/Wei_tmp_outputs/w2v_output/out.vec\",\n",
        "                        # min_count=5,\n",
        "                        batch_size=8096,\n",
        "                        iterations=epochs,\n",
        "                        num_workers=num_workers,\n",
        "                        collate_fn=collate_fn,\n",
        "                        iprint=5000,\n",
        "                        n_chunk = n_chunk,\n",
        "                        embedding_dim=100,\n",
        "                        ns_exponent=0.75,\n",
        "                        initial_lr=0.003,\n",
        "                        optimizer='sparse_adam',\n",
        "                        single_layer=True,\n",
        "                        sparse=True,\n",
        "                        lr_schedule=False,\n",
        "                        test=test,\n",
        "                        save_embedding=save_embedding,\n",
        " #                       optimizer_kwargs={'momentum':0.9},\n",
        " #                       warm_start_model = 'gdrive/My Drive/Projects with Wei/Wei_tmp_outputs/torch_model/model.pkl',\n",
        "                        )\n",
        "  wt.train()\n",
        "  return wt"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2n2Q4Kc-cRMT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PK-F-EAyNxS1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "00496d7c-38ad-4787-f6a0-b3c016196821"
      },
      "source": [
        "train_model(epochs=10,\n",
        "            num_workers=3, \n",
        "            test = False, \n",
        "            save_embedding=True)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initializing negative samples\n",
            "\n",
            "\n",
            "\n",
            "Iteration: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  5%|         | 1/20 [01:07<21:23, 67.56s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 14.323855716760006\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 15%|        | 3/20 [03:20<18:58, 66.98s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 13.435222632375664\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 25%|       | 5/20 [05:32<16:37, 66.52s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 12.697803212135529\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 35%|      | 7/20 [07:50<14:42, 67.91s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 12.054555195814618\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 50%|     | 10/20 [10:17<09:39, 57.94s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 11.473128522306222\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 60%|    | 12/20 [12:28<07:39, 57.43s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 10.939373604027764\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 70%|   | 14/20 [14:14<05:30, 55.05s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 10.439173241298244\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 85%| | 17/20 [16:43<02:47, 55.93s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 9.978357343779834\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 95%|| 19/20 [19:00<00:57, 57.09s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 9.5494693616547\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|| 20/20 [19:38<00:00, 58.91s/it]\n",
            "100%|| 20/20 [19:51<00:00, 59.59s/it]\n",
            "100%|| 20/20 [19:51<00:00, 59.59s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 9.454343617941943\n",
            "\n",
            "\n",
            "\n",
            "Iteration: 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  5%|         | 1/20 [01:07<21:16, 67.21s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 7.270135008009866\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 15%|        | 3/20 [03:40<20:23, 71.99s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 7.437938366648144\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 25%|       | 5/20 [06:13<18:33, 74.26s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 7.376213917523336\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 35%|      | 7/20 [08:59<17:13, 79.47s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 7.250039020392067\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 50%|     | 10/20 [11:52<11:16, 67.60s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 7.1055318064573365\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 60%|    | 12/20 [14:14<08:38, 64.84s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 6.95413613402606\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 70%|   | 14/20 [16:15<06:14, 62.38s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 6.8019409809673395\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 85%| | 17/20 [19:00<03:05, 61.96s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 6.657257701188173\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 95%|| 19/20 [21:30<01:03, 63.41s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 6.515159354469031\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|| 20/20 [22:13<00:00, 66.67s/it]\n",
            "100%|| 20/20 [22:27<00:00, 67.36s/it]\n",
            "100%|| 20/20 [22:27<00:00, 67.36s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 6.481034267989179\n",
            "\n",
            "\n",
            "\n",
            "Iteration: 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  5%|         | 1/20 [01:07<21:20, 67.38s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 5.427674799324943\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 15%|        | 3/20 [03:40<20:23, 71.99s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 5.540873895806929\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 25%|       | 5/20 [06:13<18:35, 74.34s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 5.535782753300877\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 35%|      | 7/20 [08:59<17:11, 79.38s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 5.496601388794009\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 50%|     | 10/20 [11:50<11:12, 67.23s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 5.448907265879279\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 60%|    | 12/20 [14:10<08:34, 64.27s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 5.394139599292353\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 70%|   | 14/20 [16:10<06:10, 61.73s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 5.336185561823647\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 85%| | 17/20 [18:53<03:03, 61.17s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 5.278964819883629\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 95%|| 19/20 [21:22<01:02, 62.84s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 5.219960350479677\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|| 20/20 [22:05<00:00, 66.27s/it]\n",
            "100%|| 20/20 [22:18<00:00, 66.95s/it]\n",
            "100%|| 20/20 [22:18<00:00, 66.95s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 5.20450094692358\n",
            "\n",
            "\n",
            "\n",
            "Iteration: 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  5%|         | 1/20 [01:07<21:13, 67.02s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 4.60655130891994\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 15%|        | 3/20 [03:39<20:20, 71.81s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 4.687594547416225\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 25%|       | 5/20 [06:12<18:31, 74.09s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 4.692307740213881\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 35%|      | 7/20 [08:58<17:10, 79.27s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 4.675118250920866\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 50%|     | 10/20 [11:49<11:12, 67.21s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 4.654605164981155\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 60%|    | 12/20 [14:10<08:33, 64.24s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 4.62884139852198\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 70%|   | 14/20 [16:09<06:10, 61.75s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 4.6005122251257795\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 85%| | 17/20 [18:52<03:03, 61.14s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 4.57187667622727\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 95%|| 19/20 [21:22<01:03, 63.09s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 4.541445735271807\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|| 20/20 [22:05<00:00, 66.28s/it]\n",
            "100%|| 20/20 [22:19<00:00, 66.99s/it]\n",
            "100%|| 20/20 [22:19<00:00, 66.99s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 4.532700010496221\n",
            "\n",
            "\n",
            "\n",
            "Iteration: 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  5%|         | 1/20 [01:07<21:25, 67.66s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 4.152862899909035\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 15%|        | 3/20 [03:40<20:26, 72.17s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 4.214297745716494\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 25%|       | 5/20 [06:13<18:33, 74.23s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 4.220182444217973\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 35%|      | 7/20 [08:58<17:09, 79.19s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 4.210041602306518\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 50%|     | 10/20 [11:49<11:10, 67.07s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 4.1994940422063705\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 60%|    | 12/20 [14:09<08:33, 64.20s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 4.1848760038301664\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 70%|   | 14/20 [16:09<06:10, 61.69s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 4.1684889145572805\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 85%| | 17/20 [18:52<03:03, 61.20s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 4.151689205631132\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 95%|| 19/20 [21:21<01:02, 62.87s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 4.1333854501356235\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|| 20/20 [22:04<00:00, 66.21s/it]\n",
            "100%|| 20/20 [22:17<00:00, 66.89s/it]\n",
            "100%|| 20/20 [22:17<00:00, 66.89s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 4.127506061380839\n",
            "\n",
            "\n",
            "\n",
            "Iteration: 6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  5%|         | 1/20 [01:06<21:12, 66.96s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.864214949388622\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 15%|        | 3/20 [03:41<20:26, 72.16s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.913795700394469\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 25%|       | 5/20 [06:13<18:33, 74.22s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.9192999949689415\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 35%|      | 7/20 [08:59<17:10, 79.26s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.912308593397098\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 50%|     | 10/20 [11:50<11:11, 67.13s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.9063348995030527\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 60%|    | 12/20 [14:10<08:34, 64.32s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.897213501559769\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 70%|   | 14/20 [16:10<06:10, 61.81s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.886737456964708\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 85%| | 17/20 [18:53<03:03, 61.28s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.876028581751338\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 95%|| 19/20 [21:23<01:03, 63.01s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.8640618205172297\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|| 20/20 [22:06<00:00, 66.32s/it]\n",
            "100%|| 20/20 [22:20<00:00, 67.01s/it]\n",
            "100%|| 20/20 [22:20<00:00, 67.01s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.859726692546475\n",
            "\n",
            "\n",
            "\n",
            "Iteration: 7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  5%|         | 1/20 [01:07<21:14, 67.08s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.6640732684850974\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 15%|        | 3/20 [03:40<20:21, 71.86s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.706555882076586\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 25%|       | 5/20 [06:12<18:32, 74.15s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.7117596629388823\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 35%|      | 7/20 [08:57<17:07, 79.05s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.706771553189665\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 50%|     | 10/20 [11:49<11:11, 67.14s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.7035141354765844\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 60%|    | 12/20 [14:09<08:33, 64.24s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.6977417194901365\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 70%|   | 14/20 [16:09<06:11, 61.87s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.690683475484297\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 85%| | 17/20 [18:51<03:03, 61.15s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.6837590766493666\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 95%|| 19/20 [21:22<01:03, 63.07s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.675675248073798\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|| 20/20 [22:04<00:00, 66.25s/it]\n",
            "100%|| 20/20 [22:18<00:00, 66.93s/it]\n",
            "100%|| 20/20 [22:18<00:00, 66.93s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.6722070054848537\n",
            "\n",
            "\n",
            "\n",
            "Iteration: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  5%|         | 1/20 [01:07<21:13, 67.02s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.5185345444824287\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 15%|        | 3/20 [03:40<20:23, 71.99s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.5565561740952805\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 25%|       | 5/20 [06:13<18:31, 74.13s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.5616681822750134\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 35%|      | 7/20 [08:59<17:11, 79.37s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.5580361616779848\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 50%|     | 10/20 [11:50<11:11, 67.18s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.5565340702877504\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 60%|    | 12/20 [14:10<08:34, 64.36s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.5529265278650395\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 70%|   | 14/20 [16:10<06:11, 61.94s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.5480217102421947\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 85%| | 17/20 [18:55<03:04, 61.59s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.543531236459154\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 95%|| 19/20 [21:26<01:03, 63.46s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.53791350505262\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|| 20/20 [22:09<00:00, 66.46s/it]\n",
            "100%|| 20/20 [22:22<00:00, 67.14s/it]\n",
            "100%|| 20/20 [22:22<00:00, 67.14s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.534939964146746\n",
            "\n",
            "\n",
            "\n",
            "Iteration: 9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  5%|         | 1/20 [01:08<21:32, 68.03s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.4087095134954226\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 15%|        | 3/20 [03:43<20:38, 72.84s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.4436642876322714\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 25%|       | 5/20 [06:18<18:48, 75.24s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.4486871455818027\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 35%|      | 7/20 [09:06<17:23, 80.25s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.445980563167437\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 50%|     | 10/20 [12:01<11:24, 68.47s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.4455525528130044\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 60%|    | 12/20 [14:23<08:42, 65.28s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.4433306536815618\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 70%|   | 14/20 [16:24<06:15, 62.61s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.4398042990858193\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 85%| | 17/20 [19:09<03:05, 61.97s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.436870934612393\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 95%|| 19/20 [21:40<01:03, 63.44s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.43282375979464\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|| 20/20 [22:23<00:00, 67.16s/it]\n",
            "100%|| 20/20 [22:36<00:00, 67.85s/it]\n",
            "100%|| 20/20 [22:36<00:00, 67.85s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.4301514205901134\n",
            "\n",
            "\n",
            "\n",
            "Iteration: 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  5%|         | 1/20 [01:07<21:21, 67.44s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.3225940221947967\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 15%|        | 3/20 [03:20<18:58, 66.97s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.355327387131139\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 25%|       | 5/20 [05:33<16:41, 66.74s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.360205908204342\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 35%|      | 7/20 [07:51<14:44, 68.07s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.358094715740478\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 50%|     | 10/20 [10:19<09:42, 58.25s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.358339708822515\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 60%|    | 12/20 [12:28<07:38, 57.27s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.3569855371309485\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 70%|   | 14/20 [14:14<05:29, 54.91s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.3543254552585275\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 85%| | 17/20 [16:43<02:46, 55.61s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.3524024918375086\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 95%|| 19/20 [18:56<00:56, 56.16s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.349394866896453\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|| 20/20 [19:33<00:00, 58.65s/it]\n",
            "100%|| 20/20 [19:46<00:00, 59.33s/it]\n",
            "100%|| 20/20 [19:46<00:00, 59.33s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 3.3468876963998517\n",
            "Saving embeddings...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.WikiTrainer at 0x7f51a6994470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXxANF5_1kZx",
        "colab_type": "text"
      },
      "source": [
        "Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqIpBwsaz95a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "6dccb84a-f78c-43f9-9c46-eba69337a3c8"
      },
      "source": [
        "%lprun -f WikiDataset.__next__ -f WikiDataset.collate -f WikiTrainer.train train_model(num_workers=0, test = True, n_chunk=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initializing negative samples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Iteration: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 1/1 [00:58<00:00, 58.34s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Loss: 5.873366421972997\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nidrvvWqRpd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "c5c81998-90f6-47b3-d82c-737368aca4ad"
      },
      "source": [
        "length = 0\n",
        "for c in read_link_pairs_chunks(n_chunk=10):\n",
        "    print(len(c))\n",
        "    length += len(c)\n",
        "print(length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "reading link pairs in 10 chunks\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 10%|         | 1/10 [00:11<01:46, 11.87s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "37397160\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|        | 2/10 [00:22<01:32, 11.62s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "37397160\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 30%|       | 3/10 [00:34<01:21, 11.61s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "37397160\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|      | 4/10 [00:45<01:08, 11.36s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "37397160\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|     | 5/10 [00:56<00:56, 11.20s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "37397160\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|    | 6/10 [01:06<00:44, 11.02s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "37397160\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 70%|   | 7/10 [01:17<00:32, 10.86s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "37397160\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|  | 8/10 [01:27<00:21, 10.69s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "37397160\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 90%| | 9/10 [01:37<00:10, 10.56s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "37397160\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|| 10/10 [01:48<00:00, 10.80s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "37397113\n",
            "373971553\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SECPcELX5soV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ContrastiveLoss(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Contrastive loss function.\n",
        "    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, margin=2.0):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, outputs, label):\n",
        "        outputs1, output2 = outputs\n",
        "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
        "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
        "                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
        "        return loss_contrastive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04bGxFzBpSEW",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpopIKro8sEP",
        "colab_type": "text"
      },
      "source": [
        "We'll define a little function to create our model and optimizer so we\n",
        "can reuse it in the future."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmvPEWOl1dQQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model(C, device):\n",
        "    model = two_tower().to(device)\n",
        "    return model, optim.Adam(model.parameters(), lr = C.LEARNING_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCcDDtLp8c6W",
        "colab_type": "text"
      },
      "source": [
        "Since we go through a similar process twice of calculating the loss for both the training set and the validation set, let's make that into its own function, loss_batch, which computes the loss for one batch.\n",
        "\n",
        "We pass an optimizer in for the training set, and use it to perform backprop. For the validation set, we don't pass an optimizer, so the method doesn't perform backprop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpkja9iG8dXc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
        "    loss = loss_func(model(xb), yb)\n",
        "\n",
        "    if opt is not None:\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "\n",
        "    return loss.item(), len(xb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ln7HPiX49KvU",
        "colab_type": "text"
      },
      "source": [
        "fit runs the necessary operations to train our model and compute the training and validation losses for each epoch. \n",
        "\n",
        "(Note that we always call model.train() before training, and model.eval() before inference, because these are used by layers such as nn.BatchNorm2d and nn.Dropout to ensure appropriate behaviour for these different phases.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5-WaMDi9MZu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit(epochs, model, loss_func, opt, train_dl, valid_dl, device):\n",
        "    counter = []\n",
        "    loss_history = [] \n",
        "    iteration_number= 0\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for i, data in enumerate(train_dl, 0):\n",
        "            xb, yb = data\n",
        "            loss_batch(model, loss_func, xb, yb, opt)\n",
        "            \n",
        "            if i %10 == 0 :\n",
        "            print(\"Epoch number {}\\n Current loss {}\\n\".format(epoch, loss_contrastive.data[0]))\n",
        "            iteration_number +=10\n",
        "            counter.append(iteration_number)\n",
        "            loss_history.append(loss_contrastive.data[0])\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            losses, nums = zip(\n",
        "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n",
        "            )\n",
        "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
        "        print(epoch, val_loss)\n",
        "\n",
        "    show_plot(counter,loss_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HlIOs-J_iQu",
        "colab_type": "text"
      },
      "source": [
        "\"get_data\" returns dataloaders for the training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKA_xlqf_kfS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TODO: send the batches to device\n",
        "\n",
        "def get_data(X, Y, C):\n",
        "    # Datasets\n",
        "    X = # IDs\n",
        "    Y = # Labels\n",
        "\n",
        "    #dataloader -> iterator\n",
        "    training_set = Dataset(X['train'], Y)\n",
        "    validation_set = Dataset(X['validation'], Y)\n",
        "\n",
        "    return (\n",
        "        DataLoader(train_ds, **C.data_params),\n",
        "        DataLoader(valid_ds, **C.data_params),\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBlkZxnhAY9l",
        "colab_type": "text"
      },
      "source": [
        "main(). \n",
        "\n",
        "Now, our whole process of obtaining the data loaders and fitting the model can be run in 3 lines of code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePoHD2d9_8hQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dl, valid_dl = get_data(X, Y, Config)\n",
        "model, opt = get_model(Config, device)\n",
        "fit(Config.NUM_EPOCHS, model, loss_func, opt, train_dl, valid_dl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lxgkl5uT1mxx",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "model, opt = get_model()\n",
        "loss_func  = ContrastiveLoss()\n",
        "print(loss(model(xb), yb))\n",
        "\n",
        "\n",
        "counter = []\n",
        "loss_history = [] \n",
        "iteration_number= 0\n",
        "\n",
        "for epoch in range(0, Config.NUM_EPOCHS):\n",
        "    model.train()\n",
        "    for i, xb1, xb2 in enumerate(train_dataloader, 0):\n",
        "        pred1, pred2 = model(xb1, xb2)\n",
        "        loss_contrastive = loss_func(pred1, pred2, yb)\n",
        "        loss_contrastive.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "  \n",
        "        if i %10 == 0 :\n",
        "            print(\"Epoch number {}\\n Current loss {}\\n\".format(epoch, loss_contrastive.data[0]))\n",
        "            iteration_number +=10\n",
        "            counter.append(iteration_number)\n",
        "            loss_history.append(loss_contrastive.data[0])\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        valid_loss = sum(loss_func(model(xb1, xb2), yb) for xb1, xb2, yb in validation_generator)\n",
        "    print(epoch, valid_loss / len(validation_generator))     \n",
        "       \n",
        "show_plot(counter,loss_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvfUzen-6Stx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}